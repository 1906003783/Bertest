[2022-10-24 05:07:33] - INFO: 成功导入BERT配置文件 /root/autodl-tmp/Bertest/bert_dna/config.json
[2022-10-24 05:07:33] - INFO:  ### 将当前配置打印到日志文件中 
[2022-10-24 05:07:33] - INFO: ### project_dir = /root/autodl-tmp/Bertest
[2022-10-24 05:07:33] - INFO: ### dataset_dir = /root/autodl-tmp/Bertest/data/dnabert
[2022-10-24 05:07:33] - INFO: ### pretrained_model_dir = /root/autodl-tmp/Bertest/bert_dna
[2022-10-24 05:07:33] - INFO: ### train_file_path = /root/autodl-tmp/Bertest/data/dnabert/hap_train.txt
[2022-10-24 05:07:33] - INFO: ### val_file_path = /root/autodl-tmp/Bertest/data/dnabert/hap_test.txt
[2022-10-24 05:07:33] - INFO: ### test_file_path = /root/autodl-tmp/Bertest/data/dnabert/hap_test.txt
[2022-10-24 05:07:33] - INFO: ### data_name = dnabert
[2022-10-24 05:07:33] - INFO: ### vocab_path = /root/autodl-tmp/Bertest/bert_dna/vocab.txt
[2022-10-24 05:07:33] - INFO: ### device = cpu
[2022-10-24 05:07:33] - INFO: ### model_save_dir = /root/autodl-tmp/Bertest/cache
[2022-10-24 05:07:33] - INFO: ### logs_save_dir = /root/autodl-tmp/Bertest/logs
[2022-10-24 05:07:33] - INFO: ### model_save_path = /root/autodl-tmp/Bertest/cache/model_dnabert.bin
[2022-10-24 05:07:33] - INFO: ### writer = <torch.utils.tensorboard.writer.SummaryWriter object at 0x7ff5fbe29850>
[2022-10-24 05:07:33] - INFO: ### is_sample_shuffle = True
[2022-10-24 05:07:33] - INFO: ### use_embedding_weight = True
[2022-10-24 05:07:33] - INFO: ### batch_size = 16
[2022-10-24 05:07:33] - INFO: ### max_sen_len = None
[2022-10-24 05:07:33] - INFO: ### pad_index = 0
[2022-10-24 05:07:33] - INFO: ### random_state = 2022
[2022-10-24 05:07:33] - INFO: ### learning_rate = 4e-05
[2022-10-24 05:07:33] - INFO: ### weight_decay = 0.1
[2022-10-24 05:07:33] - INFO: ### masked_rate = 0.15
[2022-10-24 05:07:33] - INFO: ### masked_token_rate = 0.8
[2022-10-24 05:07:33] - INFO: ### masked_token_unchanged_rate = 0.5
[2022-10-24 05:07:33] - INFO: ### log_level = 10
[2022-10-24 05:07:33] - INFO: ### use_torch_multi_head = False
[2022-10-24 05:07:33] - INFO: ### epochs = 200
[2022-10-24 05:07:33] - INFO: ### model_val_per_epoch = 1
[2022-10-24 05:07:33] - INFO: ### vocab_size = 9
[2022-10-24 05:07:33] - INFO: ### hidden_size = 768
[2022-10-24 05:07:33] - INFO: ### num_hidden_layers = 12
[2022-10-24 05:07:33] - INFO: ### num_attention_heads = 12
[2022-10-24 05:07:33] - INFO: ### hidden_act = gelu
[2022-10-24 05:07:33] - INFO: ### intermediate_size = 3072
[2022-10-24 05:07:33] - INFO: ### pad_token_id = 0
[2022-10-24 05:07:33] - INFO: ### hidden_dropout_prob = 0.1
[2022-10-24 05:07:33] - INFO: ### attention_probs_dropout_prob = 0.1
[2022-10-24 05:07:33] - INFO: ### max_position_embeddings = 512
[2022-10-24 05:07:33] - INFO: ### type_vocab_size = 2
[2022-10-24 05:07:33] - INFO: ### initializer_range = 0.02
[2022-10-24 05:07:33] - INFO: ### layer_norm_eps = 1e-12
[2022-10-24 05:07:33] - INFO: ### model_type = bert
[2022-10-24 05:07:33] - INFO: ### pooler_type = first_token_transform
